# Natural Language Processing Models

NLP has a **long history**, even before we had huge GPUs and the recent emergence of chatGPT.
One of the earliest methods is called n-grams, where we look for word (string) matches and calculate probabilities so that the computer speaks human language somewhat fluently.
Then, we have neural networks that take over the earliest simplest methods as we **gain larger computation power**. 
The evolution of neural networks was very rapid; RNN/LSTM represents many developed forms. As we gain so much more computation power thanks to computer architecture developers and the invention of attention models thanks to machine learning researchers, we now have generative models represented by GPT models.
I will dig into **n-grams** (and their neural network expression), **RNN/LSTM**, and **attention mechanisms** in this article. 

## n-grams
A key concept in n-grams is token match: the number of tokens in the source that matches the input tokens.

## RNN/LSTM
RNN stands for Recurrent Neural Networks.
LSTM stands for Long-Short Term Memory.

## Attention mechanisms
Attention models are one of the most significant inventions that impacted natural language processing.
